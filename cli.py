import click
import sys
from pathlib import Path
from logger import TaskFlowLogger
from parser import PipelineParser
from scheduler import TaskScheduler
from executor import TaskExecutor, TaskStatus


@click.group()
@click.version_option()
def cli():
    """TaskFlow â€” Ð¼Ð¸Ð½Ð¸-Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ‚Ð¾Ñ€ Ð·Ð°Ð´Ð°Ñ‡"""
    pass


@cli.command()
@click.argument('pipeline_file', type=click.Path(exists=True))
@click.option('--max-workers', '-w', default=4, help='ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ñ‹Ñ… Ð·Ð°Ð´Ð°Ñ‡')
@click.option('--retries', '-r', default=0, help='ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð¾Ðº Ð¿Ñ€Ð¸ Ð¿Ð°Ð´ÐµÐ½Ð¸Ð¸')
@click.option('--resume', is_flag=True, help='ÐŸÑ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ñ‚ÑŒ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ñ Ð¼ÐµÑÑ‚Ð° Ð¿Ð°Ð´ÐµÐ½Ð¸Ñ')
def run(pipeline_file, max_workers, retries, resume):
    """Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð°"""
    logger = TaskFlowLogger()
    logger.info("ðŸš€ TaskFlow starting...")
    logger.info(f"Pipeline: {pipeline_file}")
    logger.info(f"Max workers: {max_workers}, Retries: {retries}, Resume: {resume}")
    
    try:
        # ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½Ð°
        parser = PipelineParser(logger)
        tasks = parser.parse(pipeline_file)
        
        # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¿Ð»Ð°Ð½Ð° Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ
        scheduler = TaskScheduler(logger)
        execution_levels = scheduler.get_execution_order(tasks)
        
        # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð¸ÑÐ¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»Ñ
        executor = TaskExecutor(logger, max_workers, retries)
        
        # Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾
        if resume:
            executor.load_state()
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ° Ð·Ð°Ð´Ð°Ñ‡
        task_map = {task.id: task for task in tasks}
        
        # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð¿Ð¾ ÑƒÑ€Ð¾Ð²Ð½ÑÐ¼
        total_failed = 0
        for level_idx, level_task_ids in enumerate(execution_levels):
            logger.info(f"\n--- Level {level_idx + 1} ---")
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸
            can_execute_level = True
            for task_id in level_task_ids:
                task = task_map[task_id]
                for dep in task.depends_on:
                    dep_result = executor.results.get(dep)
                    if not dep_result or dep_result.status != TaskStatus.SUCCESS:
                        logger.error(f"Cannot execute {task_id}: dependency {dep} not successful")
                        can_execute_level = False
            
            if not can_execute_level:
                logger.error("Stopping execution due to failed dependencies")
                break
            
            # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ
            level_tasks = [task_map[task_id] for task_id in level_task_ids]
            level_results = executor.execute_level(level_tasks)
            
            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑƒÑ€Ð¾Ð²Ð½Ñ
            level_failed = sum(1 for r in level_results if r.status == TaskStatus.FAILED)
            total_failed += level_failed
            
            if level_failed > 0:
                logger.error(f"Level {level_idx + 1}: {level_failed} tasks failed")
                if level_failed == len(level_results):
                    logger.error("All tasks in level failed. Stopping execution.")
                    break
        
        # Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°
        summary = executor.get_status_summary()
        logger.info("\n=== Execution Summary ===")
        for status, count in summary.items():
            if count > 0:
                logger.info(f"{status}: {count}")
        
        if total_failed > 0:
            logger.error(f"\nâŒ Pipeline completed with {total_failed} failed tasks")
            sys.exit(1)
        else:
            logger.info("\nâœ… Pipeline completed successfully!")
            # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ñ„Ð°Ð¹Ð» ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð¿Ñ€Ð¸ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾Ð¼ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ð¸
            if executor.state_file.exists():
                executor.state_file.unlink()
            
    except Exception as e:
        logger.error(f"âŒ Pipeline failed: {e}")
        sys.exit(1)


@cli.command()
def status():
    """ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ ÑÑ‚Ð°Ñ‚ÑƒÑ Ð·Ð°Ð´Ð°Ñ‡ Ð¸Ð· Ð¿Ð¾ÑÐ»ÐµÐ´Ð½ÐµÐ³Ð¾ Ð·Ð°Ð¿ÑƒÑÐºÐ°"""
    logger = TaskFlowLogger()
    
    executor = TaskExecutor(logger)
    if not executor.load_state():
        logger.info("No previous execution state found")
        return
    
    if not executor.results:
        logger.info("No task results found")
        return
    
    logger.info("=== Task Status ===")
    for task_id, result in executor.results.items():
        status_icon = {
            'SUCCESS': 'âœ…',
            'FAILED': 'âŒ', 
            'RUNNING': 'ðŸ”„',
            'PENDING': 'â³',
            'RETRYING': 'ðŸ”„'
        }.get(result.status.value, 'â“')
        
        info = f"{status_icon} {task_id}: {result.status.value}"
        if result.attempts > 0:
            info += f" (attempts: {result.attempts})"
        
        logger.info(info)
    
    summary = executor.get_status_summary()
    logger.info("\n=== Summary ===")
    for status, count in summary.items():
        if count > 0:
            logger.info(f"{status}: {count}")


if __name__ == '__main__':
    cli()